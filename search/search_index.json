{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ZeroFS CSI Driver","text":"<p>This project is a Kubernetes CSI driver for ZeroFS: it presents S3-backed storage as a block device, formats it (e.g. <code>btrfs</code>, <code>ext4</code>, <code>xfs</code>), and mounts it for workloads.</p>"},{"location":"#what-this-driver-does","title":"What this driver does","text":"<ul> <li>Provisions a per-volume ZeroFS pod in the <code>zerofs</code> namespace (co-located with the workload node).</li> <li>Uses ZeroFS\u2019 9P + NBD sockets to create a sparse \u201cdisk file\u201d, attach it as an NBD device, and mount a filesystem on top.</li> <li>Manages lifecycle: mount/unmount, NBD attach/detach, and cleanup of per-volume resources.</li> </ul>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>0 \u2192 1 test run: Quickstart</li> <li>Start here: Getting Started</li> <li>Understand the flow: Architecture</li> <li>Configure S3 + filesystem: Configuration</li> <li>Run benchmarks/soak tests: Validation &amp; Benchmarks</li> <li>Production guidance: Operations</li> </ul>"},{"location":"#support-expectations","title":"Support &amp; expectations","text":"<p>This repo is actively evolving. Treat this documentation as the \u201csource of truth\u201d for how this driver is intended to be built, deployed, and validated.</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#high-level-lifecycle","title":"High-level lifecycle","text":"<p>When a workload mounts a PVC provisioned by this driver:</p> <ol> <li>Controller provisions the PV/PVC using the configured <code>StorageClass</code>.</li> <li>NodePublishVolume runs on the node where the workload is scheduled.</li> <li>The node plugin creates a per-volume ZeroFS pod in the <code>zerofs</code> namespace on the same node.</li> <li>The node plugin mounts ZeroFS\u2019 9P export to a staging path, creates a sparse <code>.nbd/disk</code> file, then unmounts 9P.</li> <li>The node plugin attaches <code>.nbd/disk</code> via NBD (<code>nbd-client</code>) as <code>/dev/nbdX</code>.</li> <li>If the device is fresh, it runs <code>mkfs.&lt;filesystem&gt;</code> and then mounts the filesystem to the pod\u2019s kubelet mountpoint.</li> </ol> <p>On unpublish/unmount, these steps are reversed: unmount filesystem, detach NBD, and delete the per-volume pod + secret.</p>"},{"location":"architecture/#why-9p-nbd","title":"Why 9P + NBD?","text":"<ul> <li>9P gives a POSIX-ish interface for creating and manipulating the backing disk file.</li> <li>NBD turns that backing file into a kernel block device so normal filesystems and workloads can use it.</li> </ul>"},{"location":"architecture/#kubernetes-objects-created","title":"Kubernetes objects created","text":"<p>Typical objects involved: - <code>Deployment/zerofs-csi-controller</code> + <code>DaemonSet/zerofs-csi-node</code> in <code>zerofs</code> - Per-volume <code>Pod/zerofs-volume-pvc-...</code> in <code>zerofs</code> - Per-volume <code>Secret/zerofs-config-pvc-...</code> in <code>zerofs</code> (generated config)</p>"},{"location":"architecture/#known-constraints","title":"Known constraints","text":"<ul> <li>Host nodes must expose <code>/dev/nbd*</code> and support <code>nbd</code> (the validation harness mounts host <code>/dev</code> into k3d nodes).</li> <li>Tail latency can be dominated by S3 endpoint behavior; treat durability-sensitive workloads as \u201cbenchmark first\u201d.</li> </ul>"},{"location":"benchmarks/","title":"Validation &amp; Benchmarks","text":""},{"location":"benchmarks/#mirrored-zerofsbench-suite","title":"Mirrored <code>ZeroFS/bench</code> suite","text":"<p>The mirrored suite matches the one used on zerofs.net. It is useful for relative comparisons, but it is not durability-sensitive (it does not fsync per operation).</p> <p>Run:</p> <pre><code>make bench\n</code></pre>"},{"location":"benchmarks/#durability-sensitive-fio-benchmarks","title":"Durability-sensitive fio benchmarks","text":"<p>For an external MinIO/S3 endpoint, create a <code>StorageClass</code> that references secrets by name (see <code>validation/zerofs-external-config.yaml</code>), then run:</p> <pre><code>make test-external        # btrfs (zerofs-external)\nmake test-external-ext4   # ext4  (zerofs-external-ext4)\n</code></pre> <p>These fio jobs include a prefill step so later random reads don\u2019t hit unwritten extents.</p>"},{"location":"benchmarks/#reference-results-external-minio","title":"Reference results (external MinIO)","text":"<p>These numbers are not a guarantee. They are provided to set expectations and to help detect regressions. Your results will vary with S3 latency/throttling, node CPU/memory, and filesystem choice.</p> <p>Environment (example run on 2025-12-18): - k3d cluster, private S3-compatible endpoint (bucket redacted) - ZeroFS <code>ghcr.io/barre/zerofs:0.22.4</code></p> <p>Durability-sensitive fio (4k + fsync) (higher is better, lower latency is better):</p> StorageClass Job Read IOPS Write IOPS p99 clat <code>zerofs-external</code> (btrfs) <code>randread_4k_iodepth32</code> ~335 0 ~379ms <code>zerofs-external</code> (btrfs) <code>randwrite_4k_fsync1</code> 0 ~24.8 ~5.6ms <code>zerofs-external-ext4</code> (ext4) <code>randread_4k_iodepth32</code> ~1.5 0 ~14.3s <code>zerofs-external-ext4</code> (ext4) <code>randwrite_4k_fsync1</code> 0 ~0.18 ~2.4s <p>Soak (15 minutes): <code>randrw</code> 4k, <code>--fsync=16</code>, with a 2-minute injected S3 outage mid-run: - ~2.8 read IOPS / ~1.2 write IOPS, p99 clat ~3.7s</p> <p>Interpretation: - Filesystem choice matters. In this environment, <code>btrfs</code> was dramatically better than <code>ext4</code> for fsync-heavy IO. - Tail latency can spike into seconds during endpoint brownouts/outages (expected for durability-sensitive workloads on S3).</p>"},{"location":"benchmarks/#soak-testing","title":"Soak testing","text":"<p>Use the long-running Job to catch tail-latency and stability issues:</p> <pre><code>make soak\n</code></pre> <p>The soak manifest is <code>validation/fio-soak.yaml</code> (defaults to <code>zerofs-test</code> and runs for 1 hour).</p>"},{"location":"benchmarks/#nbd-leak-regression-attachdetach","title":"NBD leak regression (attach/detach)","text":"<p>To prevent <code>/dev/nbd*</code> leaks on repeated kubelet retries and attach/detach cycles, the repo includes a regression harness that: - repeatedly mounts/unmounts a PVC on a single node - asserts the number of connected NBD devices returns to the baseline each iteration</p> <p>Run:</p> <pre><code>ITERATIONS=50 make nbd-regression\n</code></pre> <p>Failure injection mode (deletes the per-volume ZeroFS pod mid-attach):</p> <pre><code>ITERATIONS=50 make nbd-regression-chaos\n</code></pre> <p>The node plugin also runs a best-effort startup NBD reconciler: it detaches devices that are connected but not mounted. This helps recover from abrupt restarts, but it will not touch mounted volumes.</p>"},{"location":"benchmarks/#collect-logs","title":"Collect logs","text":"<pre><code>make logs\n</code></pre> <p>This writes <code>validation-logs.txt</code> with controller/node logs and benchmark outputs.</p>"},{"location":"ci/","title":"CI &amp; Releases","text":""},{"location":"ci/#docker-image-build-workflow","title":"Docker image build workflow","text":"<p><code>.github/workflows/build.yml</code> builds/pushes Docker images to Docker Hub:</p> <ul> <li><code>halceon/zerofs-csi-driver:latest</code> on <code>main</code> pushes</li> <li><code>halceon/zerofs-csi-driver:vX.Y.Z</code> (plus <code>:vX.Y</code> and <code>:vX</code>) on tag pushes</li> </ul> <p>The workflow expects Docker Hub credentials via repository secrets: - <code>DOCKERHUB_USERNAME</code> - <code>DOCKERHUB_TOKEN</code></p>"},{"location":"ci/#weekly-upstream-zerofs-checks","title":"Weekly upstream ZeroFS checks","text":"<p><code>.github/workflows/zerofs-upstream.yaml</code> runs weekly (and on demand):</p> <ol> <li>Reads the latest <code>Barre/ZeroFS</code> GitHub release.</li> <li>Compares it to the version currently configured in <code>validation/zerofs-config.yaml</code>.</li> <li>If newer, builds/pushes <code>halceon/zerofs-csi-driver:vX.Y.Z</code> and <code>:latest</code>, then validates with k3d + in-cluster MinIO.</li> </ol> <p>Notes: - Validation uses <code>make setup-infra</code>, <code>make deploy IMAGE_NAME=\u2026</code>, and <code>make test</code>. - The workflow patches the in-cluster <code>ConfigMap</code> to set <code>zerofsImage=ghcr.io/barre/zerofs:X.Y.Z</code> for the run.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>This driver uses a <code>ConfigMap</code> referenced by the <code>StorageClass</code> to build the ZeroFS <code>config.toml</code>. Secrets are referenced by name (do not inline credentials).</p>"},{"location":"configuration/#in-cluster-minio-example","title":"In-cluster MinIO example","text":"<p>See <code>validation/zerofs-config.yaml</code>.</p> <ul> <li><code>Secret/minio-zerofs-credentials</code> (in <code>zerofs</code>):</li> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>Secret/zerofs-encryption-password</code>:</li> <li><code>password</code></li> <li><code>ConfigMap/zerofs-btrfs-config</code>:</li> <li><code>awsEndpoint</code>: <code>http://minio.default.svc.cluster.local:9000</code></li> <li><code>storageURL</code>: <code>s3://zerofs</code></li> <li><code>zerofsImage</code>: <code>ghcr.io/barre/zerofs:X.Y.Z</code></li> </ul>"},{"location":"configuration/#external-minio-s3-compatible-endpoint","title":"External MinIO / S3-compatible endpoint","text":"<p>See <code>validation/zerofs-external-config.yaml</code>:</p> <ul> <li><code>awsAllowHTTP: \"false\"</code> (TLS)</li> <li><code>awsEndpoint: https://\u2026</code></li> <li><code>storageURL: s3://&lt;bucket&gt;</code></li> <li><code>awsCredentialsSecretName</code>: a <code>Secret</code> containing <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code></li> </ul>"},{"location":"configuration/#storageclass-parameters","title":"StorageClass parameters","text":"<p>Example:</p> <pre><code>parameters:\n  filesystem: btrfs\n  configMapName: zerofs-btrfs-config\n  deleteDataOnPVCDelete: \"false\"\nmountOptions:\n  - compress-force=zstd\n</code></pre> <p>Supported filesystems depend on what <code>mkfs.*</code> tools are available in the driver container. The <code>validation/</code> configs currently exercise <code>btrfs</code> and <code>ext4</code>.</p>"},{"location":"configuration/#deletedataonpvcdelete-optional","title":"<code>deleteDataOnPVCDelete</code> (optional)","text":"<p>If set to <code>\"true\"</code>, the controller will attempt to delete the S3 prefix for the volume during <code>DeleteVolume</code>.</p> <p>Notes: - This is off by default in <code>validation/</code> (<code>\"false\"</code>). - The prefix is computed as: <code>storageURL</code> + <code>/&lt;volumeID&gt;</code> (same layout used by the per-volume ZeroFS config). - Deletion errors cause <code>DeleteVolume</code> to fail (so Kubernetes will retry).</p>"},{"location":"configuration/#rustlog-optional","title":"<code>rustLog</code> (optional)","text":"<p>The per-volume ZeroFS pod can be configured with a custom <code>RUST_LOG</code> value via the config <code>ConfigMap</code>:</p> <pre><code>data:\n  rustLog: \"zerofs=info,slatedb=info\"\n</code></pre> <p>This can be useful when investigating fencing/restart issues or reducing log volume in production.</p>"},{"location":"configuration/#podreadytimeoutseconds-optional","title":"<code>podReadyTimeoutSeconds</code> (optional)","text":"<p>The node plugin waits for the per-volume ZeroFS pod to become Ready (9P socket created) before proceeding with 9P/NBD setup. On slower clusters or cold starts, a longer timeout reduces first-attach flakes.</p> <pre><code>data:\n  podReadyTimeoutSeconds: \"120\"\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li><code>kubectl</code></li> <li><code>k3d</code></li> <li>Linux host with NBD support (example):</li> </ul> <pre><code>sudo modprobe nbd max_part=100\nls -l /dev/nbd0\n</code></pre>"},{"location":"getting-started/#local-end-to-end-validation-k3d-minio","title":"Local end-to-end validation (k3d + MinIO)","text":"<p>From the repo root:</p> <pre><code>make setup-infra\nmake build IMAGE_NAME=halceon/zerofs-csi-driver:latest\nmake deploy IMAGE_NAME=halceon/zerofs-csi-driver:latest\nmake test\nmake bench\nmake logs\n</code></pre>"},{"location":"getting-started/#kustomize-install-optional","title":"Kustomize install (optional)","text":"<p>To install the driver via Kustomize:</p> <pre><code>kubectl apply -k deploy/\n</code></pre> <p>To deploy the validation stack (MinIO + driver + StorageClass):</p> <pre><code>kubectl apply -k validation/\n</code></pre> <p>Artifacts: - <code>validation-logs.txt</code> (aggregated controller/node/benchmark logs)</p>"},{"location":"getting-started/#tear-down","title":"Tear down","text":"<pre><code>make teardown\n</code></pre>"},{"location":"getting-started/#notes","title":"Notes","text":"<ul> <li>The validation manifests live in <code>validation/</code> (MinIO, driver install, fio, mirrored <code>ZeroFS/bench</code>).</li> <li>The in-cluster driver deployment uses <code>imagePullPolicy: Never</code> and imports images via <code>k3d image import</code>.</li> </ul>"},{"location":"metrics/","title":"Metrics","text":"<p>The node plugin exposes Prometheus metrics on <code>:8080</code> (same port as controller-runtime metrics). These are intended for debugging performance and spotting NBD lifecycle regressions.</p>"},{"location":"metrics/#scrape-inspect","title":"Scrape / inspect","text":"<pre><code>kubectl -n zerofs get pods -l app=zerofs-csi-node -o name\nkubectl -n zerofs port-forward pod/&lt;zerofs-csi-node-pod&gt; 8080:8080\ncurl -s localhost:8080/metrics | rg '^zerofs_csi_'\n</code></pre>"},{"location":"metrics/#exported-metrics","title":"Exported metrics","text":""},{"location":"metrics/#zerofs_csi_node_operation_seconds","title":"<code>zerofs_csi_node_operation_seconds</code>","text":"<p>Histogram. Labels: - <code>operation</code>: <code>NodePublishVolume</code> | <code>NodeUnpublishVolume</code> - <code>stage</code>: a coarse step name (e.g. <code>create_pod</code>, <code>mount_9p</code>, <code>create_disk</code>, <code>nbd_connect</code>, <code>mkfs</code>, <code>mount_fs</code>, <code>unmount_fs</code>, <code>nbd_disconnect</code>, <code>remove_pod</code>)</p> <p>Use this to track \u201cwhere time goes\u201d during attach/detach and to alert on tail latency spikes.</p>"},{"location":"metrics/#zerofs_csi_node_errors_total","title":"<code>zerofs_csi_node_errors_total</code>","text":"<p>Counter. Labels: - <code>operation</code>: <code>NodePublishVolume</code> | <code>NodeUnpublishVolume</code> - <code>stage</code>: same as above</p> <p>Use this to quickly identify which step fails most frequently (auth issues, endpoint timeouts, mkfs failures, etc.).</p>"},{"location":"metrics/#suggested-queries-promql","title":"Suggested queries (PromQL)","text":"<pre><code># p99 publish latency by stage (5m window)\nhistogram_quantile(0.99, sum by (le, stage) (rate(zerofs_csi_node_operation_seconds_bucket{operation=\"NodePublishVolume\"}[5m])))\n\n# error rate by stage\nsum by (operation, stage) (rate(zerofs_csi_node_errors_total[5m]))\n</code></pre>"},{"location":"operations/","title":"Operations","text":""},{"location":"operations/#troubleshooting-quick-checks","title":"Troubleshooting quick checks","text":"<pre><code>kubectl -n zerofs get pods -o wide\nkubectl -n zerofs logs -l app=zerofs-csi-node --all-containers --tail=200\nkubectl -n zerofs logs -l app=zerofs-csi-controller --all-containers --tail=200\nkubectl get events --sort-by=.lastTimestamp | tail -n 50\n</code></pre> <p>Metrics (Prometheus format) (see also Metrics):</p> <pre><code>kubectl -n zerofs get pods -l app=zerofs-csi-node -o name\nkubectl -n zerofs port-forward pod/&lt;zerofs-csi-node-pod&gt; 8080:8080\ncurl -s localhost:8080/metrics | rg '^zerofs_csi_'\n</code></pre>"},{"location":"operations/#common-failure-modes","title":"Common failure modes","text":"<ul> <li>No <code>/dev/nbd*</code> devices: ensure the host supports NBD and (for k3d) <code>/dev</code> is mounted into nodes.</li> <li>Slow mounts / timeouts: often tied to S3 endpoint latency or throttling; reproduce with <code>make soak</code> and inspect tail latencies.</li> <li>Credentials / bucket issues: confirm secrets exist in <code>zerofs</code> and that the bucket is reachable/created.</li> <li>Fencing / restarts (HA MinIO): if the per-volume ZeroFS pod repeatedly restarts with fencing errors, treat it as a multi-component issue (ZeroFS + S3/MinIO + config). Start by lowering log verbosity (<code>rustLog</code>) and capturing a soak run against the HA endpoint.</li> <li>NBD leak symptoms (connected <code>/dev/nbd*</code> steadily climbs): reproduce with <code>make nbd-regression</code> / <code>make nbd-regression-chaos</code>, then inspect node logs for cleanup warnings.</li> </ul>"},{"location":"operations/#ha-minio-fencing-mitigation-checklist","title":"HA MinIO / fencing mitigation checklist","text":"<p>This driver can\u2019t \u201cfix\u201d fencing behavior by itself, but you can reduce churn and improve diagnosability:</p> <ul> <li>Set <code>rustLog</code> in the config <code>ConfigMap</code> (e.g. <code>zerofs=info,slatedb=info</code>) and only enable debug logs when needed.</li> <li>Prefer stable, low-latency networking to the MinIO endpoint; measure with <code>make soak</code>.</li> <li>Consider enabling the Envoy sidecar only as an explicit mitigation when your S3 endpoint is flaky (note: retries can affect fencing guarantees).</li> </ul>"},{"location":"operations/#security-configuration-tips","title":"Security &amp; configuration tips","text":"<ul> <li>Never commit credentials; reference <code>Secret</code> names and create them out-of-band.</li> <li>Prefer TLS endpoints (<code>awsAllowHTTP: \"false\"</code>) for external storage.</li> <li>Use least-privilege IAM policies: bucket-scoped access and only the required object actions.</li> </ul>"},{"location":"operations/#production-readiness-checklist","title":"Production readiness checklist","text":"<ul> <li>Resource requests/limits: set explicit CPU/memory for controller, node, and per-volume pods.</li> <li>Observability: scrape <code>/metrics</code> and alert on tail latency + error rates (e.g. <code>zerofs_csi_node_operation_seconds</code>, <code>zerofs_csi_node_errors_total</code>).</li> <li>Chaos/failure testing: restart nodes, kill ZeroFS pods, simulate endpoint outages.</li> <li>Upgrade strategy: pin <code>zerofsImage</code> versions and roll forward with canaries.</li> </ul>"},{"location":"quickstart/","title":"Quickstart (0 \u2192 1 Test Run)","text":"<p>This guide gets you from a clean machine to a full end-to-end validation using k3d + in-cluster MinIO.</p>"},{"location":"quickstart/#1-prereqs","title":"1) Prereqs","text":"<ul> <li>Docker</li> <li><code>kubectl</code></li> <li><code>k3d</code></li> <li>Linux host with NBD support:</li> </ul> <pre><code>sudo modprobe nbd max_part=100\nls -l /dev/nbd0\n</code></pre>"},{"location":"quickstart/#2-clone-and-build","title":"2) Clone and build","text":"<pre><code>git clone https://github.com/trisongz/zerofs-csi-driver.git\ncd zerofs-csi-driver\n</code></pre>"},{"location":"quickstart/#3-run-the-full-local-validation","title":"3) Run the full local validation","text":"<p>This sequence: - creates a k3d cluster named <code>zerofs-test</code> - deploys MinIO + a test bucket - builds &amp; pushes the driver image (defaults to <code>halceon/zerofs-csi-driver:latest</code>) - imports the image into k3d and deploys the driver - runs fio and prints output - gathers logs into <code>validation-logs.txt</code></p> <pre><code>make setup-infra\nmake build IMAGE_NAME=halceon/zerofs-csi-driver:latest\nmake deploy IMAGE_NAME=halceon/zerofs-csi-driver:latest\nmake test\nmake logs\n</code></pre>"},{"location":"quickstart/#4-sanity-checks","title":"4) Sanity checks","text":"<pre><code>kubectl get nodes -o wide\nkubectl -n zerofs get pods -o wide\nkubectl get sc\n</code></pre> <p>Troubleshooting:</p> <pre><code>kubectl -n zerofs logs -l app=zerofs-csi-node --all-containers --tail=200\nkubectl -n zerofs logs -l app=zerofs-csi-controller --all-containers --tail=200\n</code></pre>"},{"location":"quickstart/#5-clean-up","title":"5) Clean up","text":"<pre><code>make teardown\n</code></pre>"},{"location":"quickstart/#optional-benchmarks","title":"Optional: benchmarks","text":"<ul> <li>Mirrored <code>ZeroFS/bench</code> suite: <code>make bench</code></li> <li>Long-running soak test: <code>make soak</code></li> <li>NBD leak regression: <code>ITERATIONS=20 make nbd-regression</code> (or <code>make nbd-regression-chaos</code>)</li> </ul>"}]}